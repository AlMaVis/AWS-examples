{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#aws-examples","title":"AWS-examples","text":"<p>A comprehensive codebase containing AWS examples curated during my preparation for the AWS Certified Solutions Architect \u2013 Associate certification. This repository includes practical hands-on commands and scripts using:</p> <ul> <li>AWS CLI \u2014 managing AWS services via command line</li> <li>AWS SDKs \u2014 examples using various programming languages (to be added)</li> <li>Terraform Infrastructure as Code (IaC) \u2014 defining AWS resources declaratively</li> <li>Other relevant tools and scripts related to AWS solutions architecture</li> </ul>"},{"location":"#purpose","title":"Purpose","text":"<p>This repo serves as a personal reference and practice ground for mastering AWS concepts and services required by the Solutions Architect Associate exam. It showcases real-world AWS CLI commands, policy JSONs, Terraform snippets, and more \u2014 all organized by key topics aligned with the AWS Certified Solutions Architect exam domains.</p>"},{"location":"#contents","title":"Contents","text":"<ul> <li>ACLs: Managing Access Control Lists with S3</li> <li>Checksums: Understanding object integrity and checksum algorithms</li> <li>Metadata: Working with object metadata and tags in S3</li> <li>Prefixes: Using S3 prefixes and folder-like structures</li> <li>Storage Classes: Uploading and managing S3 objects with different storage classes</li> <li>(More topics coming soon, including IAM, EC2, VPC, Lambda, CloudFormation, and more)</li> </ul> <p>Each topic contains detailed commands, JSON examples, output samples, and notes to reinforce learning.</p>"},{"location":"#why-this-repo","title":"Why This Repo?","text":"<ul> <li>Hands-on Focus: Learn by doing with exact CLI commands and real outputs.</li> <li>Exam-Driven: Topics aligned with exam objectives and key AWS services.</li> <li>Reference Ready: Clear examples to revisit when preparing for the exam or designing solutions.</li> <li>Multi-Tool Coverage: AWS CLI, SDK snippets, and Terraform examples all in one place.</li> </ul>"},{"location":"#how-to-use-this-repo","title":"How to Use This Repo","text":"<ul> <li>Clone or fork the repository to your local environment.</li> <li>Navigate into each topic folder for specific AWS examples.</li> <li>Run commands on your own AWS account or a sandbox environment (ensure proper permissions and cost awareness).</li> <li>Modify and extend examples to deepen your understanding or prepare custom scenarios.</li> </ul>"},{"location":"#prerequisites","title":"Prerequisites","text":"<ul> <li>AWS CLI installed and configured with valid credentials.</li> <li>Basic understanding of AWS core services and concepts.</li> <li>Optional: Terraform installed for IaC examples.</li> <li>Recommended: An AWS free tier or sandbox account to safely test examples.</li> </ul>"},{"location":"#important-notes","title":"Important Notes","text":"<ul> <li>Some commands may incur costs; always review AWS pricing and clean up resources after use.</li> <li>This repository is for study purposes only \u2014 not production-ready code.</li> <li>Always follow best security practices when managing credentials and AWS resources.</li> </ul>"},{"location":"#resources-references","title":"Resources &amp; References","text":"<ul> <li>AWS Certified Solutions Architect \u2013 Associate Exam Guide</li> <li>AWS CLI Documentation</li> <li>Amazon S3 Developer Guide</li> <li>Terraform AWS Provider</li> <li>AWS Well-Architected Framework</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions, suggestions, and feedback are welcome! If you find issues or want to add new examples, feel free to submit a pull request or open an issue.</p>"},{"location":"#contact","title":"Contact","text":"<p>Feel free to reach out if you have questions or want to discuss AWS certification topics.</p> <p>Happy Studying! \ud83d\ude80</p>"},{"location":"api/sts/","title":"AWS STS AssumeRole Hands-On Reference","text":"<p>This document summarizes a hands-on experience with AWS STS AssumeRole for temporary cross-account or cross-user access. The steps and commands are collected as a reference for future AWS projects and AWS Solutions Architect certification preparation. For detailed explanations, refer to the official AWS documentation linked below.</p>"},{"location":"api/sts/#user-and-credentials-preparation","title":"User and Credentials Preparation","text":"<ul> <li> <p>Create a user with no permissions and generate access keys: <code>sh   aws iam create-user --user-name sts-example-user   aws iam create-access-key --user-name sts-example-user --output table</code> Creating IAM Users</p> </li> <li> <p>Configure the credentials: <code>sh   aws configure</code>   Edit your AWS credentials file to use a named profile (e.g., <code>sts</code>).</p> </li> <li> <p>Test identity and permissions: <code>sh   aws sts get-caller-identity --profile sts   aws s3 ls --profile sts</code>   Expect an AccessDenied error for S3, since no permissions are attached.</p> </li> </ul>"},{"location":"api/sts/#role-and-resource-setup","title":"Role and Resource Setup","text":"<ul> <li> <p>Deploy the CloudFormation stack to create a role and S3 bucket: <code>sh   chmod u+x bin/deploy   ./bin/deploy</code> AWS CloudFormation Documentation</p> </li> <li> <p>Role and bucket are defined in <code>iam-role.yaml</code>.   The role allows the user to assume it and grants S3 access to the specified bucket.</p> </li> </ul>"},{"location":"api/sts/#policy-attachment","title":"Policy Attachment","text":"<ul> <li>Attach an inline policy to allow the user to assume the role: <code>sh   aws iam put-user-policy \\     --user-name sts-example-user \\     --policy-name StsPolicy \\     --policy-document file://policy.json</code> IAM Policies</li> </ul>"},{"location":"api/sts/#assume-role-and-use-temporary-credentials","title":"Assume Role and Use Temporary Credentials","text":"<ul> <li> <p>Get the role name from CloudFormation: <code>sh   aws cloudformation describe-stack-resources --stack-name sts-example</code></p> </li> <li> <p>Assume the role using STS: <code>sh   aws sts assume-role \\     --role-arn arn:aws:iam::&lt;account-id&gt;:role/&lt;role-name&gt; \\     --role-session-name s3-sts-example \\     --profile sts</code> AWS STS AssumeRole</p> </li> <li> <p>Set the returned credentials as a new profile (<code>assumed</code>) in your AWS credentials file.</p> </li> <li> <p>Verify the assumed identity: <code>sh   aws sts get-caller-identity --profile assumed</code></p> </li> <li> <p>Access the S3 bucket with the assumed role: <code>sh   aws s3 ls s3://sts-example-bucket-001 --profile assumed</code></p> </li> </ul>"},{"location":"api/sts/#key-points","title":"Key Points","text":"<ul> <li>Demonstrates the use of IAM users, roles, and policies for secure, temporary access.</li> <li>Shows how to use AWS STS to assume a role and access resources with temporary credentials.</li> <li>CloudFormation is used for reproducible resource and role setup.</li> </ul>"},{"location":"api/sts/#cleanup","title":"Cleanup","text":"<p>To remove all resources created during this hands-on:</p> <ul> <li> <p>Delete the CloudFormation stack (removes the role and S3 bucket): <code>sh   aws cloudformation delete-stack --stack-name sts-example</code></p> </li> <li> <p>Delete the IAM user: <code>sh   aws iam delete-user-policy --user-name sts-example-user --policy-name StsPolicy   aws iam delete-access-key --user-name sts-example-user --access-key-id AKIARMEKEFD7XUFCOEZC   aws iam delete-user --user-name sts-example-user</code></p> </li> <li> <p>Remove any local AWS CLI credentials/profiles you created for this test.</p> </li> </ul>"},{"location":"api/sts/#references","title":"References","text":"<ul> <li>AWS STS AssumeRole</li> <li>IAM Roles</li> <li>IAM Policies</li> <li>AWS CloudFormation</li> <li>AWS CLI S3 Reference</li> </ul>"},{"location":"s3/acls/","title":"ACLs","text":"<p>This repository contains practical commands and examples illustrating the use of AWS S3 Access Control Lists (ACLs) and related bucket configurations. These snippets reflect real-world usage with <code>awscli</code> (<code>s3api</code>), supporting an understanding of S3 access management relevant to the AWS Solutions Architect Associate certification.</p>"},{"location":"s3/acls/#create-a-new-bucket","title":"Create a new bucket","text":"<pre><code>aws s3api create-bucket --bucket acl-example-001 --region us-east-1\n</code></pre> <p>Creates an S3 bucket named <code>acl-example-001</code> in the <code>us-east-1</code> region using the low-level <code>s3api</code> command for full control over API parameters. AWS CLI Docs: create-bucket</p>"},{"location":"s3/acls/#turn-off-block-public-access-for-acls","title":"Turn off Block Public Access for ACLs","text":"<pre><code>aws s3api put-public-access-block \\\n--bucket acl-example-001 \\\n--public-access-block-configuration \"BlockPublicAcls=false,IgnorePublicAcls=false,RestrictPublicBuckets=true,BlockPublicPolicy=true\"\n</code></pre> <p>Modifies the Public Access Block settings to allow ACL-based public access while restricting public bucket policies. This is important to enable ACLs to function without being blocked by global bucket settings. AWS CLI Docs: put-public-access-block AWS Docs: S3 Block Public Access</p>"},{"location":"s3/acls/#show-public-access-block-configuration","title":"Show Public Access Block configuration","text":"<pre><code>aws s3api get-public-access-block --bucket acl-example-001\n</code></pre> <pre><code>{\n  \"PublicAccessBlockConfiguration\": {\n    \"BlockPublicAcls\": false,\n    \"IgnorePublicAcls\": false,\n    \"BlockPublicPolicy\": true,\n    \"RestrictPublicBuckets\": true\n  }\n}\n</code></pre> <p>Retrieves current public access block settings on the bucket to verify ACL and policy restrictions.</p>"},{"location":"s3/acls/#change-bucket-ownership-controls","title":"Change Bucket Ownership Controls","text":"<pre><code>aws s3api put-bucket-ownership-controls \\\n--bucket acl-example-001 \\\n--ownership-controls \"Rules=[{ObjectOwnership=BucketOwnerPreferred}]\"\n</code></pre> <p>Applies ownership control settings so that objects uploaded by other AWS accounts are automatically owned by the bucket owner, simplifying access management. AWS CLI Docs: put-bucket-ownership-controls AWS Docs: Object Ownership</p>"},{"location":"s3/acls/#show-ownership-controls","title":"Show Ownership Controls","text":"<pre><code>aws s3api get-bucket-ownership-controls --bucket acl-example-001\n</code></pre> <pre><code>{\n  \"OwnershipControls\": {\n    \"Rules\": [\n      {\n        \"ObjectOwnership\": \"BucketOwnerPreferred\"\n      }\n    ]\n  }\n}\n</code></pre> <p>Confirms the applied ownership controls on the bucket.</p>"},{"location":"s3/acls/#change-acls-to-allow-a-user-in-another-aws-account","title":"Change ACLs to allow a user in another AWS Account","text":"<pre><code>aws s3api put-bucket-acl \\\n  --bucket acl-example-001 \\\n  --access-control-policy file://acl-policy.json\n</code></pre> <p>Updates the bucket ACL using a JSON file (<code>acl-policy.json</code>) that defines grants for external AWS accounts or users. This is essential for cross-account access using ACLs. AWS CLI Docs: put-bucket-acl AWS Docs: Using ACLs</p>"},{"location":"s3/acls/#show-the-modified-bucket-acl","title":"Show the Modified Bucket ACL","text":"<pre><code>aws s3api get-bucket-acl --bucket acl-example-001\n</code></pre> <pre><code>{\n  \"Grants\": [\n    {\n      \"Grantee\": {\n        \"ID\": \"*** Grantee-Canonical-User-ID ***\",\n        \"Type\": \"CanonicalUser\"\n      },\n      \"Permission\": \"FULL_CONTROL\"\n    }\n  ],\n  \"Owner\": {\n    \"ID\": \"*** Owner-Canonical-User-ID ***\"\n  }\n}\n</code></pre> <p>Displays the current ACL settings on the bucket, showing all grants including external users with their permissions. The canonical user IDs uniquely identify AWS accounts/users.</p>"},{"location":"s3/acls/#clean-up","title":"Clean Up","text":"<pre><code>aws s3 rb s3://acl-example-001\n</code></pre>"},{"location":"s3/acls/#additional-resources","title":"Additional Resources","text":"<ul> <li>AWS CLI Reference: s3api</li> <li>Amazon S3 ACL Concepts</li> <li>AWS Solutions Architect Associate Exam Guide</li> </ul>"},{"location":"s3/bucket-policies/","title":"Bucket Policies","text":"<p>This example demonstrates how to use S3 bucket policies to:</p> <ul> <li>Grant object-level access to a specific IAM user</li> <li>Deny access to a specific folder (e.g. <code>MySecretFolder/</code>)</li> <li>Keep public access completely blocked</li> </ul>"},{"location":"s3/bucket-policies/#create-the-bucket","title":"\ud83e\udea3 Create the Bucket","text":"<pre><code>aws s3 mb s3://bucket-policy-example-001\n````\n\n---\n\n## \ud83d\udd12 Block Public Access\n\nEnsure that public access is disabled for the bucket:\n\n```sh\naws s3api put-public-access-block \\\n  --bucket bucket-policy-example-001 \\\n  --public-access-block-configuration \\\n    BlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=true,RestrictPublicBuckets=true\n</code></pre>"},{"location":"s3/bucket-policies/#policy-grant-access-to-iam-user-except-a-folder","title":"\ud83d\udc64 Policy: Grant Access to IAM User (Except a Folder)","text":"<p>We'll create a policy that:</p> <ul> <li>Allows object actions (<code>GetObject</code>, <code>PutObject</code>, <code>DeleteObject</code>) to user <code>aws-examples-restricted</code></li> <li>Denies access to the path <code>MySecretFolder/*</code> within the bucket</li> </ul>"},{"location":"s3/bucket-policies/#bucket-policyjson","title":"<code>bucket-policy.json</code>","text":"<pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"AllowRestrictedUserFullAccess\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"arn:aws:iam::123456789012:user/aws-examples-restricted\"\n      },\n      \"Action\": [\n        \"s3:GetObject\",\n        \"s3:PutObject\",\n        \"s3:DeleteObject\"\n      ],\n      \"Resource\": \"arn:aws:s3:::bucket-policy-example-001/*\"\n    },\n    {\n      \"Sid\": \"DenyRestrictedUserSecretFolder\",\n      \"Effect\": \"Deny\",\n      \"Principal\": {\n        \"AWS\": \"arn:aws:iam::123456789012:user/aws-examples-restricted\"\n      },\n      \"Action\": [\n        \"s3:GetObject\",\n        \"s3:PutObject\",\n        \"s3:DeleteObject\"\n      ],\n      \"Resource\": \"arn:aws:s3:::bucket-policy-example-001/MySecretFolder/*\"\n    }\n  ]\n}\n</code></pre> <p>\ud83d\udccc Deny always takes precedence over Allow in AWS IAM evaluation logic.</p>"},{"location":"s3/bucket-policies/#apply-the-policy","title":"\ud83d\ude80 Apply the Policy","text":"<pre><code>aws s3api put-bucket-policy \\\n  --bucket bucket-policy-example-001 \\\n  --policy file://bucket-policy.json\n</code></pre>"},{"location":"s3/bucket-policies/#verify-bucket-policy","title":"\ud83d\udcc4 Verify Bucket Policy","text":"<pre><code>aws s3api get-bucket-policy \\\n  --bucket bucket-policy-example-001 \\\n  --query Policy \\\n  --output text | jq .\n</code></pre> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"AllowRestrictedUserFullAccess\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"arn:aws:iam::123456789012:user/aws-examples-restricted\"\n      },\n      \"Action\": [\n        \"s3:GetObject\",\n        \"s3:PutObject\",\n        \"s3:DeleteObject\"\n      ],\n      \"Resource\": \"arn:aws:s3:::bucket-policy-example-001/*\"\n    },\n    {\n      \"Sid\": \"DenyRestrictedUserSecretFolder\",\n      \"Effect\": \"Deny\",\n      \"Principal\": {\n        \"AWS\": \"arn:aws:iam::123456789012:user/aws-examples-restricted\"\n      },\n      \"Action\": [\n        \"s3:GetObject\",\n        \"s3:PutObject\",\n        \"s3:DeleteObject\"\n      ],\n      \"Resource\": \"arn:aws:s3:::bucket-policy-example-001/MySecretFolder/*\"\n    }\n  ]\n}\n</code></pre>"},{"location":"s3/bucket-policies/#clean-up","title":"\ud83e\uddfc Clean Up","text":"<pre><code>aws s3 rb s3://bucket-policy-example-001 --force\n</code></pre>"},{"location":"s3/bucket-policies/#references","title":"\ud83d\udcda References","text":"<ul> <li>S3 Bucket Policy Examples</li> <li>S3 Block Public Access</li> <li>IAM Policy Evaluation Logic</li> </ul>"},{"location":"s3/checksums/","title":"Checksums","text":"<p>This document demonstrates the use of AWS S3 with checksum operations, showcasing checksum verification during uploads and metadata retrieval using AWS CLI commands. The examples reflect hands-on experience relevant to S3 data integrity concepts tested in the AWS Solutions Architect Associate certification.</p>"},{"location":"s3/checksums/#create-a-new-s3-bucket","title":"Create a new S3 bucket","text":"<pre><code>aws s3 mb s3://checksums-examples-00012\n</code></pre> <p>Creates a new bucket named <code>checksums-examples-00012</code> using the higher-level <code>aws s3</code> command. AWS CLI Docs: mb (make bucket)</p>"},{"location":"s3/checksums/#create-a-file-to-perform-checksum-on","title":"Create a file to perform checksum on","text":"<pre><code>echo \"checksum\" &gt; myfile.txt\n</code></pre> <p>Creates a local file <code>myfile.txt</code> with the content <code>\"checksum\"</code> for checksum demonstration.</p>"},{"location":"s3/checksums/#calculate-md5-checksum-of-the-file","title":"Calculate MD5 checksum of the file","text":"<pre><code>md5sum myfile.txt\n</code></pre> <pre><code>bd4a9b642562547754086de2dab26b7d  myfile.txt\n</code></pre> <p>Computes the MD5 checksum locally for verification purposes. This checksum is used by S3 as the default <code>ETag</code> for single-part uploads without server-side encryption. Note: MD5 is not always reliable for multipart uploads or encrypted files.</p>"},{"location":"s3/checksums/#upload-the-file-to-s3-and-verify","title":"Upload the file to S3 and verify","text":"<pre><code>aws s3 cp myfile.txt s3://checksums-examples-00012\naws s3api head-object --bucket checksums-examples-00012 --key myfile.txt\n</code></pre> <pre><code>{\n    \"AcceptRanges\": \"bytes\",\n    \"ContentLength\": 9,\n    \"ETag\": \"\\\"bd4a9b642562547754086de2dab26b7d\\\"\",\n    \"ContentType\": \"text/plain\",\n    \"ServerSideEncryption\": \"AES256\",\n    \"Metadata\": {}\n}\n</code></pre> <p>Uploads the file using <code>aws s3 cp</code>, a simplified command that handles transfers. Then retrieves object metadata with <code>s3api head-object</code> showing the <code>ETag</code> (MD5 checksum for this object), encryption status, and content length. AWS CLI Docs: s3 cp AWS CLI Docs: s3api head-object</p>"},{"location":"s3/checksums/#calculate-sha1-checksum-locally","title":"Calculate SHA1 checksum locally","text":"<pre><code>openssl dgst -sha1 -binary myfile.txt | base64\n</code></pre> <pre><code>Ll2vAgHd6waKYtXgjaGGV6ssa+k=  myfile.txt\n</code></pre> <p>Uses OpenSSL to generate a binary SHA1 digest of the file, then encodes it in Base64 format \u2014 the format required by S3 for specifying checksums on upload. This demonstrates how to prepare checksums other than MD5 for S3's checksum API.</p>"},{"location":"s3/checksums/#upload-a-file-with-sha1-checksum-validation","title":"Upload a file with SHA1 checksum validation","text":"<pre><code>aws s3api put-object \\\n  --bucket checksums-examples-00012 \\\n  --key myfilesha1.txt \\\n  --body myfile.txt \\\n  --checksum-algorithm SHA1 \\\n  --checksum-sha1 Ll2vAgHd6waKYtXgjaGGV6ssa+k=\n</code></pre> <pre><code>{\n    \"ETag\": \"\\\"bd4a9b642562547754086de2dab26b7d\\\"\",\n    \"ChecksumSHA1\": \"Ll2vAgHd6waKYtXgjaGGV6ssa+k=\",\n    \"ChecksumType\": \"FULL_OBJECT\",\n    \"ServerSideEncryption\": \"AES256\"\n}\n</code></pre> <p>Uploads the file while explicitly specifying the SHA1 checksum, allowing S3 to verify data integrity during the transfer. The response confirms the checksum stored alongside the object metadata. This feature improves data validation beyond the default MD5 ETag. AWS CLI Docs: put-object with checksum AWS Docs: S3 Checksum Support</p>"},{"location":"s3/checksums/#clean-up-resources","title":"Clean up resources","text":"<pre><code>aws s3 rm \"s3://checksums-examples-00012/\" --recursive --output text\naws s3api delete-bucket --bucket checksums-examples-00012\n</code></pre> <p>Deletes all objects in the bucket recursively, then deletes the bucket itself to avoid resource sprawl. Using <code>aws s3 rm</code> for bulk deletion and <code>s3api delete-bucket</code> to remove the bucket. AWS CLI Docs: s3 rm AWS CLI Docs: s3api delete-bucket</p>"},{"location":"s3/checksums/#additional-notes","title":"Additional Notes","text":"<ul> <li>The default <code>ETag</code> returned by S3 is typically the MD5 checksum of the object for simple uploads but can differ for multipart or encrypted uploads.</li> <li>Using the <code>--checksum-algorithm</code> and checksum parameters in <code>put-object</code> commands ensures stronger integrity checks.</li> <li><code>aws s3api</code> commands provide granular control and access to advanced S3 features compared to the higher-level <code>aws s3</code> commands.</li> </ul>"},{"location":"s3/cors/","title":"AWS S3 CORS Hands-On Reference","text":"<p>This document summarizes a hands-on experience configuring two AWS S3 buckets for static website hosting and enabling cross-origin resource sharing (CORS) between them. It serves as a reference for future AWS projects and AWS Solutions Architect certification.</p>"},{"location":"s3/cors/#website-1-static-hosting-and-public-access","title":"Website 1: Static Hosting and Public Access","text":"<ul> <li> <p>Bucket creation: <code>sh   aws s3 mb s3://cors-example-0001</code> AWS CLI: aws s3 mb</p> </li> <li> <p>Block public access settings: <code>sh   aws s3api put-public-access-block \\     --bucket cors-example-0001 \\     --public-access-block-configuration \"BlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=false,RestrictPublicBuckets=false\"</code> S3 Block Public Access</p> </li> <li> <p>Bucket policy for public read:   See <code>bucket-policy.json</code> for the policy document.   <code>sh   aws s3api put-bucket-policy --bucket cors-example-0001 --policy file://bucket-policy.json</code> S3 Bucket Policy Examples</p> </li> <li> <p>Enable static website hosting:   See <code>website-conf.json</code> for the configuration.   <code>sh   aws s3api put-bucket-website --bucket cors-example-0001 --website-configuration file://website-conf.json</code> Hosting a Static Website on Amazon S3</p> </li> <li> <p>Upload website content: <code>sh   aws s3 cp index1.html s3://cors-example-0001   aws s3 cp error.html s3://cors-example-0001</code></p> </li> <li> <p>Website endpoint format: <code>http://cors-example-0001.s3-website.eu-west-3.amazonaws.com/</code></p> </li> </ul>"},{"location":"s3/cors/#website-2-second-static-site-for-cors-testing","title":"Website 2: Second Static Site for CORS Testing","text":"<ul> <li> <p>Bucket creation: <code>sh   aws s3 mb s3://cors-example-0002</code></p> </li> <li> <p>Block public access settings: <code>sh   aws s3api put-public-access-block \\     --bucket cors-example-0002 \\     --public-access-block-configuration \"BlockPublicAcls=true,IgnorePublicAcls=true,BlockPublicPolicy=false,RestrictPublicBuckets=false\"</code></p> </li> <li> <p>Bucket policy for public read:   See <code>bucket-policy2.json</code> for the policy document.   <code>sh   aws s3api put-bucket-policy --bucket cors-example-0002 --policy file://bucket-policy2.json</code></p> </li> <li> <p>Enable static website hosting:   See <code>website-conf2.json</code> for the configuration.   <code>sh   aws s3api put-bucket-website --bucket cors-example-0002 --website-configuration file://website-conf2.json</code></p> </li> <li> <p>Upload test page: <code>sh   aws s3 cp index2.html s3://cors-example-0002   aws s3 cp error.html s3://cors-example-0002</code></p> </li> <li> <p>Website endpoint format: <code>http://cors-example-0002.s3-website.eu-west-3.amazonaws.com/</code></p> </li> </ul>"},{"location":"s3/cors/#cors-configuration","title":"CORS Configuration","text":"<ul> <li>CORS policy for Website 1:   See <code>cors.json</code> for the configuration.   Example:   <code>json   {       \"CORSRules\": [           {               \"AllowedHeaders\": [\"*\"],               \"AllowedMethods\": [\"GET\", \"POST\", \"PUT\"],               \"AllowedOrigins\": [                   \"http://cors-example-0002.s3-website.eu-west-3.amazonaws.com\"               ],               \"ExposeHeaders\": [],               \"MaxAgeSeconds\": 3000           }       ]   }</code> S3 CORS Configuration</li> </ul> <p>Apply with:   <code>sh   aws s3api put-bucket-cors --bucket cors-example-0001 --cors-configuration file://cors.json</code></p>"},{"location":"s3/cors/#cross-origin-test","title":"Cross-Origin Test","text":"<ul> <li>index2.html (in Website 2) fetches <code>index1.html</code> from Website 1 using JavaScript <code>fetch()</code>.</li> <li>If CORS is configured correctly, the content of <code>index1.html</code> is displayed in Website 2.</li> <li>If not, browser blocks the request due to CORS policy.</li> </ul>"},{"location":"s3/cors/#clean-up","title":"Clean up","text":"<pre><code>aws s3 rb s3://cors-example-0001 --force\naws s3 rb s3://cors-example-0002 --force\n</code></pre>"},{"location":"s3/cors/#summary","title":"Summary","text":"<ul> <li>S3 static website hosting requires public read access and website configuration.</li> <li>CORS must be explicitly enabled on the source bucket for cross-origin requests.</li> <li>Testing CORS between two S3-hosted sites is a practical way to understand browser security and AWS configuration.</li> </ul>"},{"location":"s3/cors/#references","title":"References","text":"<ul> <li>Amazon S3 Static Website Hosting</li> <li>Amazon S3 CORS Documentation</li> <li>Amazon S3 Bucket Policies</li> <li>AWS CLI S3 Reference</li> </ul>"},{"location":"s3/encryption/","title":"S3 Encryption Hands-On Reference","text":"<p>This document summarizes hands-on experience with different S3 encryption options, including SSE-S3, SSE-KMS, and SSE-C. The steps and commands are collected as a reference for future AWS projects and for AWS Solutions Architect certification preparation. For detailed explanations, refer to the official AWS documentation linked in each section.</p>"},{"location":"s3/encryption/#bucket-and-file-preparation","title":"Bucket and File Preparation","text":"<ul> <li> <p>Bucket creation: <code>sh   aws s3 mb s3://encryption-examples-001</code> AWS CLI: aws s3 mb</p> </li> <li> <p>Test file creation: <code>sh   echo \"encryption file\" &gt;&gt; myfile.txt</code></p> </li> </ul>"},{"location":"s3/encryption/#standard-upload-no-encryption","title":"Standard Upload (No Encryption)","text":"<ul> <li>Put object in bucket: <code>sh   aws s3 cp myfile.txt s3://encryption-examples-001</code> Uploading Objects</li> </ul>"},{"location":"s3/encryption/#server-side-encryption-with-aws-kms-sse-kms","title":"Server-Side Encryption with AWS KMS (SSE-KMS)","text":"<ul> <li>Put object with SSE-KMS: <code>sh   aws s3api put-object \\     --bucket encryption-examples-001 \\     --key myfile.txt \\     --body myfile.txt \\     --server-side-encryption aws:kms \\     --ssekms-key-id &lt;kms-key-id&gt;</code> Using SSE-KMS</li> </ul>"},{"location":"s3/encryption/#server-side-encryption-with-customer-provided-keys-sse-c","title":"Server-Side Encryption with Customer-Provided Keys (SSE-C)","text":"<ul> <li> <p>Generate a 256-bit key and related values: <code>sh   openssl rand -out ssec.key 32   base64 ssec.key &gt; ssec.key.b64   openssl md5 -binary ssec.key | base64 &gt; ssec.key.md5</code> Using SSE-C</p> </li> <li> <p>Put object with SSE-C: <code>sh   aws s3api put-object \\     --bucket encryption-examples-001 \\     --key myfile.txt \\     --body myfile.txt \\     --sse-customer-algorithm AES256 \\     --sse-customer-key \"$(cat ssec.key.b64)\" \\     --sse-customer-key-md5 \"$(cat ssec.key.md5)\"</code></p> </li> </ul>"},{"location":"s3/encryption/#verifying-encryption","title":"Verifying Encryption","text":"<ul> <li>Check encryption metadata (SSE-C example): <code>sh   aws s3api head-object \\     --bucket encryption-examples-001 \\     --key myfile.txt \\     --sse-customer-algorithm AES256 \\     --sse-customer-key \"$(cat ssec.key.b64)\" \\     --sse-customer-key-md5 \"$(cat ssec.key.md5)\"</code> Head Object</li> </ul> <p>Example output:   <code>json   {       \"AcceptRanges\": \"bytes\",       \"LastModified\": \"2025-05-29T10:24:30+00:00\",       \"ContentLength\": 16,       \"ETag\": \"\\\"072e6b241c63c6ed93adb392********\\\"\",       \"ContentType\": \"binary/octet-stream\",       \"Metadata\": {},       \"SSECustomerAlgorithm\": \"AES256\",       \"SSECustomerKeyMD5\": \"teFot4VXcmEq+yjk********\"   }</code></p>"},{"location":"s3/encryption/#clean-up","title":"Clean Up","text":"<pre><code>aws s3 rb s3://encryption-examples-001 --force\n</code></pre>"},{"location":"s3/encryption/#key-points","title":"Key Points","text":"<ul> <li>SSE-S3, SSE-KMS, and SSE-C were tested for object encryption.</li> <li>SSE-KMS requires a valid KMS key ID.</li> <li>SSE-C requires manual key management and passing the key and its MD5 with each request.</li> <li>Encryption metadata is only visible when the correct key is provided for SSE-C.</li> </ul>"},{"location":"s3/encryption/#references","title":"References","text":"<ul> <li>Amazon S3 Encryption Overview</li> <li>Using SSE-KMS</li> <li>Using SSE-C</li> <li>AWS CLI S3 Reference</li> </ul>"},{"location":"s3/encryption-client-side/","title":"S3 Client-Side Encryption Hands-On Reference","text":"<p>This document summarizes a hands-on experience with client-side encryption for Amazon S3 using a raw AES-256 key and the AWS Encryption SDK for Python. The steps and commands are collected as a reference for future AWS projects and AWS Solutions Architect certification preparation. For detailed explanations, refer to the official AWS documentation linked below.</p>"},{"location":"s3/encryption-client-side/#key-generation-and-usage","title":"Key Generation and Usage","text":"<ul> <li> <p>Generate a 256-bit (32 bytes) random key and encode it in base64: <code>sh   openssl rand -base64 32 &gt; raw.key.b64</code></p> </li> <li> <p>Use the key with the Python script: <code>sh   python encrypt.py upload --raw-key-b64 \"$(cat raw.key.b64)\"   python encrypt.py download --raw-key-b64 \"$(cat raw.key.b64)\"</code></p> </li> </ul>"},{"location":"s3/encryption-client-side/#encryptpy-overview","title":"encrypt.py Overview","text":"<p>The <code>encrypt.py</code> script demonstrates client-side encryption and decryption using a raw AES-256 keyring with the AWS Encryption SDK for Python. - Upload mode: Encrypts the example data (<code>Hello World</code>) using the provided AES key and uploads the ciphertext to the S3 bucket <code>encryption-client-side-example-001</code> with the key <code>myfile.txt</code>. - Download mode: Downloads the ciphertext from S3, decrypts it using the same AES key, and prints the plaintext to the console.</p> <p>Key points: - Uses a raw AES-256 key (not KMS). - Demonstrates both encryption/upload and download/decryption. - Encryption context is included for demonstration.</p>"},{"location":"s3/encryption-client-side/#example-usage","title":"Example Usage","text":"<ul> <li> <p>Encrypt and upload: <code>sh   python encrypt.py upload --raw-key-b64 \"$(cat raw.key.b64)\"</code></p> </li> <li> <p>Download and decrypt: <code>sh   python encrypt.py download --raw-key-b64 \"$(cat raw.key.b64)\"</code></p> </li> </ul>"},{"location":"s3/encryption-client-side/#clean-up","title":"Clean Up","text":"<pre><code>aws s3 rb s3://encryption-client-side-example-001 --force\n</code></pre>"},{"location":"s3/encryption-client-side/#references","title":"References","text":"<ul> <li>AWS Encryption SDK for Python</li> <li>Raw AES Keyring (AWS Encryption SDK)</li> <li>Amazon S3 Documentation</li> <li>AWS CLI S3 Reference</li> </ul>"},{"location":"s3/metadata/","title":"Metadata","text":"<p>This document presents hands-on operations for managing metadata and tags on objects in Amazon S3 using AWS CLI commands. It highlights key concepts such as user metadata, object tagging, metadata immutability, and metadata-based object aggregation. This is useful for anyone working towards AWS certification or managing S3 buckets in real-world scenarios.</p>"},{"location":"s3/metadata/#create-a-new-s3-bucket","title":"Create a new S3 bucket","text":"<pre><code>aws s3 mb s3://metadata-example-001\n</code></pre> <p>Creates a bucket called <code>metadata-example-001</code> using the high-level <code>aws s3</code> CLI command. AWS CLI Docs: s3 mb</p>"},{"location":"s3/metadata/#create-a-file-with-example-content","title":"Create a file with example content","text":"<pre><code>echo \"metadata\" &gt; file.txt\n</code></pre> <p>Generates a simple text file <code>file.txt</code> to upload with metadata.</p>"},{"location":"s3/metadata/#upload-a-file-with-custom-user-metadata","title":"Upload a file with custom user metadata","text":"<pre><code>aws s3api put-object --bucket metadata-example-001 --key file.txt --body file.txt --metadata Meta=Data,Author=Alice,Env=Dev\n</code></pre> <p>Uses the granular <code>s3api put-object</code> command to upload the file with custom key-value metadata pairs. Note: Metadata keys are case-insensitive and will be returned as lowercase in responses. AWS CLI Docs: s3api put-object</p>"},{"location":"s3/metadata/#retrieve-object-metadata-using-head-object","title":"Retrieve object metadata using head-object","text":"<pre><code>aws s3api head-object --bucket metadata-example-001 --key file.txt\n</code></pre> <pre><code>{\n    \"AcceptRanges\": \"bytes\",\n    \"ContentLength\": 9,\n    \"ETag\": \"\\\"b3348b95d7571534c9a0b0fbc28bc5be\\\"\",\n    \"ContentType\": \"binary/octet-stream\",\n    \"ServerSideEncryption\": \"AES256\",\n    \"Metadata\": {\n        \"env\": \"Dev\",\n        \"author\": \"Alice\",\n        \"meta\": \"Data\"\n    }\n}\n</code></pre> <p>Returns metadata and other object properties such as encryption status and content length. Observe metadata keys appear in lowercase regardless of how they were uploaded. AWS CLI Docs: s3api head-object</p>"},{"location":"s3/metadata/#add-tags-to-an-existing-object","title":"Add tags to an existing object","text":"<pre><code>aws s3api put-object-tagging \\\n  --bucket metadata-example-001 \\\n  --key file.txt \\\n  --tagging 'TagSet=[{Key=Project,Value=Demo},{Key=Env,Value=Dev}]'\n</code></pre> <p>Adds or updates object tags \u2014 a distinct metadata mechanism used for resource management, lifecycle policies, and billing. Tags are separate from user metadata and support filtering and lifecycle rules. AWS CLI Docs: s3api put-object-tagging</p>"},{"location":"s3/metadata/#view-object-tags","title":"View object tags","text":"<pre><code>aws s3api get-object-tagging --bucket metadata-example-001 --key file.txt\n</code></pre> <pre><code>{\n    \"TagSet\": [\n        {\n            \"Key\": \"Project\",\n            \"Value\": \"Demo\"\n        },\n        {\n            \"Key\": \"Env\",\n            \"Value\": \"Dev\"\n        }\n    ]\n}\n</code></pre> <p>Retrieves the tag set assigned to the specified object. AWS CLI Docs: s3api get-object-tagging</p>"},{"location":"s3/metadata/#upload-additional-files-with-metadata-to-simulate-groupingaggregation","title":"Upload additional files with metadata (to simulate grouping/aggregation)","text":"<pre><code>echo \"metadataA\" &gt; fileA.txt\necho \"metadataB\" &gt; fileB.txt\n</code></pre> <pre><code>aws s3api put-object --bucket metadata-example-001 --key group1/fileA.txt --body fileA.txt --metadata group=1\n\naws s3api put-object --bucket metadata-example-001 --key group2/fileB.txt --body fileB.txt --metadata group=2\n</code></pre> <p>Uploads files into \u201cfolders\u201d by prefix and assigns distinct metadata to support logical grouping or filtering on the client side.</p>"},{"location":"s3/metadata/#list-objects-filtered-by-prefix","title":"List objects filtered by prefix","text":"<pre><code>aws s3api list-objects-v2 --bucket metadata-example-001 --prefix group1/\n</code></pre> <p>Retrieves all objects with the specified prefix (<code>group1/</code>), enabling a pseudo-folder structure in a flat S3 namespace. AWS CLI Docs: s3api list-objects-v2</p>"},{"location":"s3/metadata/#copy-an-object-while-replacing-metadata","title":"Copy an object while replacing metadata","text":"<pre><code>aws s3api copy-object \\\n  --bucket metadata-example-001 \\\n  --copy-source metadata-example-001/file.txt \\\n  --key file_copy.txt \\\n  --metadata Author=Bob \\\n  --metadata-directive REPLACE\n</code></pre> <p>Copies an existing object to a new key, overwriting the metadata with new values. Important: Without <code>--metadata-directive REPLACE</code>, original metadata is retained by default. AWS CLI Docs: s3api copy-object</p>"},{"location":"s3/metadata/#before-copying-check-original-metadata","title":"Before copying, check original metadata:","text":"<pre><code>aws s3api head-object --bucket metadata-example-001 --key file.txt\n</code></pre>"},{"location":"s3/metadata/#after-copying-verify-new-metadata","title":"After copying, verify new metadata:","text":"<pre><code>aws s3api head-object --bucket metadata-example-001 --key file_copy.txt\n</code></pre>"},{"location":"s3/metadata/#metadata-limitations","title":"\ud83d\udeab Metadata Limitations","text":"<ul> <li>Metadata is immutable after upload unless you overwrite or copy the object.</li> <li>Metadata values must be strings; no complex data types.</li> <li>AWS S3 does not support server-side filtering by metadata. Filtering or querying based on metadata must be done client-side after listing or downloading objects.</li> </ul>"},{"location":"s3/metadata/#clean-up-resources","title":"Clean up resources","text":"<pre><code>aws s3 rm \"s3://metadata-example-001/\" --recursive --output text\naws s3api delete-bucket --bucket metadata-example-001\n</code></pre> <p>Deletes all objects recursively then deletes the bucket to clean up. AWS CLI Docs: s3 rm AWS CLI Docs: s3api delete-bucket</p>"},{"location":"s3/prefixes/","title":"Prefixes","text":"<p>This document demonstrates how to work with S3 object keys that simulate folders using prefixes. It explores S3\u2019s key length limits and how prefixes enable a hierarchical organization in the flat S3 namespace.</p>"},{"location":"s3/prefixes/#create-a-new-s3-bucket","title":"Create a new S3 bucket","text":"<pre><code>aws s3 mb s3://prefixes-example-001\n</code></pre> <p>Creates an S3 bucket named <code>prefixes-example-001</code> using the high-level <code>aws s3</code> command. AWS CLI Docs: s3 mb</p>"},{"location":"s3/prefixes/#create-a-folder-prefix-in-the-bucket","title":"Create a folder (prefix) in the bucket","text":"<pre><code>aws s3api put-object --bucket prefixes-example-001 --key hello/\n</code></pre> <p>Creates an empty object with key ending in <code>/</code>, which acts like a folder placeholder in S3. S3 is flat, but the <code>/</code> separator in keys enables tools and consoles to display a folder-like hierarchy. AWS CLI Docs: s3api put-object</p>"},{"location":"s3/prefixes/#create-many-folders-with-a-long-key-up-to-1024-bytes","title":"Create many folders with a long key (up to 1024 bytes)","text":"<pre><code>aws s3api put-object --bucket prefixes-example-001 --key Lorem/ipsum/dolor/sit/amet/consectetur/adipiscing/elit/Proin/sed/lectus/sed/nibh/sagittis/tempus/Orci/varius/natoque/penatibus/et/magnis/dis/parturient/montes/nascetur/ridiculus/mus/Suspendisse/convallis/eros/quis/sollicitudin/porttitor/Phasellus/at/turpis/feugiat/aliquet/velit/sed/consectetur/turpis/Quisque/gravida/quam/non/ante/ultrices/eu/egestas/tortor/sollicitudin/Maecenas/aliquam/mi/lectus/non/pellentesque/magna/tincidunt/quis/Duis/sit/amet/est/vel/libero/sollicitudin/elementum/Maecenas/at/dolor/ut/erat/blandit/efficitur/Sed/efficitur/vestibulum/arcu/sed/convallis/orci/ultrices/sit/amet/Proin/feugiat/eu/libero/et/ultricies/Maecenas/nunc/nibh/gravida/in/aliquet/a/gravida/et/nisl/In/in/commodo/elit/Donec/tristique/est/vel/tristique/consequat/erat/dui/eleifend/diam/a/congue/dolor/urna/vitae/ipsum/Mauris/suscipit/mauris/quis/mauris/sagittis/dapibus/Integer/eget/lacinia/augue/Etiam/sed/velit/posuere/suscipit/quam/a/facilisis/dolor/Vestibulum/in/scelerisque/orci/ac/iaculis/biam/efficutort/velit/convallis/\n</code></pre> <pre><code>{\n    \"ETag\": \"\\\"d41d8cd98f00b204e9800998ecf8427e\\\"\",\n    \"ChecksumCRC64NVME\": \"AAAAAAAAAAA=\",\n    \"ChecksumType\": \"FULL_OBJECT\",\n    \"ServerSideEncryption\": \"AES256\"\n}\n</code></pre> <p>S3 object keys can be very long, up to 1024 bytes in length (UTF-8). This example uses a very long key with many folder-like prefixes. The empty object simulates nested folders, useful for some tools or user expectations.</p>"},{"location":"s3/prefixes/#try-to-exceed-the-1024-byte-key-limit-expect-failure","title":"Try to exceed the 1024 byte key limit (expect failure)","text":"<pre><code>aws s3api put-object --bucket prefixes-example-001 --key Lorem/ipsum/dolor/sit/amet/consectetur/adipiscing/elit/Proin/sed/lectus/sed/nibh/sagittis/tempus/Orci/varius/natoque/penatibus/et/magnis/dis/parturient/montes/nascetur/ridiculus/mus/Suspendisse/convallis/eros/quis/sollicitudin/porttitor/Phasellus/at/turpis/feugiat/aliquet/velit/sed/consectetur/turpis/Quisque/gravida/quam/non/ante/ultrices/eu/egestas/tortor/sollicitudin/Maecenas/aliquam/mi/lectus/non/pellentesque/magna/tincidunt/quis/Duis/sit/amet/est/vel/libero/sollicitudin/elementum/Maecenas/at/dolor/ut/erat/blandit/efficitur/Sed/efficitur/vestibulum/arcu/sed/convallis/orci/ultrices/sit/amet/Proin/feugiat/eu/libero/et/ultricies/Maecenas/nunc/nibh/gravida/in/aliquet/a/gravida/et/nisl/In/in/commodo/elit/Donec/tristique/est/vel/tristique/consequat/erat/dui/eleifend/diam/a/congue/dolor/urna/vitae/ipsum/Mauris/suscipit/mauris/quis/mauris/sagittis/dapibus/Integer/eget/lacinia/augue/Etiam/sed/velit/posuere/suscipit/quam/a/facilisis/dolor/Vestibulum/in/scelerisque/orci/ac/iaculis/biam/efficutort/velit/convallis/myfile.txt --body myfile.txt\n</code></pre> <pre><code>An error occurred (KeyTooLongError) when calling the PutObject operation: Your key is too long\n</code></pre> <p>Attempting to create an object key longer than 1024 bytes results in a <code>KeyTooLongError</code>. This limit applies to the entire UTF-8 encoded key string, including all prefixes and filename. AWS S3 Limits Documentation</p>"},{"location":"s3/prefixes/#clean-up-remove-all-objects-and-delete-the-bucket","title":"Clean up: Remove all objects and delete the bucket","text":"<pre><code>aws s3 rm s3://prefixes-example-001/ --recursive\naws s3api delete-bucket --bucket prefixes-example-001\n</code></pre> <p>Recursively deletes all objects and then removes the bucket to avoid lingering resources. AWS CLI Docs: s3 rm AWS CLI Docs: s3api delete-bucket</p>"},{"location":"s3/prefixes/#summary","title":"Summary","text":"<p>This exercise demonstrates how S3 treats prefixes as part of object keys to simulate folders, the maximum allowed key length of 1024 bytes, and the resulting error when exceeding this limit. </p>"},{"location":"s3/storage-classes/","title":"Storage Classes","text":"<p>This guide demonstrates creating an S3 bucket, uploading an object with different storage classes, and inspecting the storage class metadata via AWS CLI.</p>"},{"location":"s3/storage-classes/#create-a-bucket","title":"Create a bucket","text":"<pre><code>aws s3 mb s3://class-change-example-001\n</code></pre> <p>Creates an S3 bucket named <code>class-change-example-001</code>.</p>"},{"location":"s3/storage-classes/#create-a-file","title":"Create a file","text":"<pre><code>echo \"class change\" &gt;&gt; myfile.txt\n</code></pre> <p>Generates a local text file containing the string \"class change\".</p>"},{"location":"s3/storage-classes/#upload-the-file-with-default-storage-class-standard","title":"Upload the file with default storage class (STANDARD)","text":"<pre><code>aws s3 cp myfile.txt s3://class-change-example-001\naws s3api head-object --bucket class-change-example-001 --key myfile.txt\n</code></pre> <p>Example output:</p> <pre><code>{\n    \"AcceptRanges\": \"bytes\",\n    \"ContentLength\": 13,\n    \"ETag\": \"\\\"dbc3716e480a39040a7f3d324c3f791d\\\"\",\n    \"ContentType\": \"text/plain\",\n    \"ServerSideEncryption\": \"AES256\",\n    \"Metadata\": {}\n}\n</code></pre> <p>Note: The <code>StorageClass</code> field does not appear if it is the default <code>STANDARD</code> storage class. For other storage classes, it will be explicitly shown.</p>"},{"location":"s3/storage-classes/#upload-the-file-specifying-a-different-storage-class-standard_ia","title":"Upload the file specifying a different storage class (STANDARD_IA)","text":"<pre><code>aws s3 cp myfile.txt s3://class-change-example-001 --storage-class STANDARD_IA\naws s3api head-object --bucket class-change-example-001 --key myfile.txt\n</code></pre> <p>Example output:</p> <pre><code>{\n    \"AcceptRanges\": \"bytes\",\n    \"LastModified\": \"2025-05-27T22:02:05+00:00\",\n    \"ContentLength\": 13,\n    \"ETag\": \"\\\"dbc3716e480a39040a7f3d324c3f791d\\\"\",\n    \"ContentType\": \"text/plain\",\n    \"ServerSideEncryption\": \"AES256\",\n    \"Metadata\": {},\n    \"StorageClass\": \"STANDARD_IA\"\n}\n</code></pre>"},{"location":"s3/storage-classes/#common-s3-storage-classes","title":"\ud83d\udcda Common S3 Storage Classes","text":"Storage Class Description STANDARD Default; frequently accessed data STANDARD_IA Infrequent Access \u2014 lower cost for cold data ONEZONE_IA Infrequent Access stored in a single AZ INTELLIGENT_TIERING Automatically moves data between tiers GLACIER Archive storage; retrieval in minutes/hours DEEP_ARCHIVE Long-term archive; retrieval takes hours <p>See the AWS S3 Storage Classes Documentation for details.</p>"},{"location":"s3/storage-classes/#clean-up","title":"Clean up","text":"<pre><code>aws s3 rm s3://class-change-example-001/myfile.txt\naws s3 rb s3://class-change-example-001\n</code></pre> <p>Important: Ensure the bucket is empty before deleting it using <code>rb</code> (remove bucket).</p> <p>This example clearly shows how to upload objects to S3 with different storage classes, and how to verify the storage class using the <code>head-object</code> command. Adjusting storage class helps optimize cost based on access patterns.</p>"},{"location":"vpc/nacl/","title":"Terraform AWS VPC and Network ACL Example","text":"<p>This Terraform project creates a basic AWS VPC with a Network ACL (NACL) using good practices.</p>"},{"location":"vpc/nacl/#features","title":"Features","text":"<ul> <li>Creates a VPC with DNS support and hostnames enabled</li> <li>Creates a Network ACL attached to the VPC</li> <li>Adds simple rules to allow inbound and outbound HTTP (port 80) traffic</li> <li>Uses variables for flexibility</li> <li>Provides outputs for important resource IDs</li> </ul>"},{"location":"vpc/nacl/#prerequisites","title":"Prerequisites","text":"<ul> <li>Terraform &gt;= 1.12.1</li> <li>AWS CLI configured with credentials</li> <li>AWS account with permissions to create VPCs and NACLs</li> </ul>"},{"location":"vpc/nacl/#usage","title":"Usage","text":"<ol> <li> <p>Clone the repository:</p> <p><code>sh git clone &lt;repository_url&gt; cd terraform-vpc-nacl</code></p> </li> <li> <p>Initialize Terraform:</p> <p><code>sh terraform init</code></p> </li> <li> <p>Review the planned changes:</p> <p><code>sh terraform plan</code></p> </li> <li> <p>Apply the configuration:</p> <p><code>sh terraform apply</code></p> </li> <li> <p>Confirm and wait for resources to be created.</p> </li> <li> <p>Use the output values to reference your VPC and NACL IDs.</p> </li> </ol>"},{"location":"vpc/nacl/#variables","title":"Variables","text":"Name Description Default <code>aws_region</code> AWS region to deploy resources <code>us-east-1</code> <code>vpc_cidr</code> CIDR block for the VPC <code>10.0.0.0/16</code> <code>nacl_name</code> Name tag for the Network ACL <code>example-nacl</code>"},{"location":"vpc/nacl/#aws-cli-commands","title":"AWS CLI Commands","text":"<p>Inspect and verify the created resources.</p> <ol> <li> <p>List all VPCs:</p> <p><code>sh aws ec2 describe-vpcs --query \"Vpcs[*].{ID:VpcId,CIDR:CidrBlock,IsDefault:IsDefault}\" --output table</code></p> </li> <li> <p>List all NACLs:</p> <p><code>sh aws ec2 describe-network-acls --query \"NetworkAcls[*].{ID:NetworkAclId,VPC:VpcId,IsDefault:IsDefault}\" --output table</code></p> </li> <li> <p>Describe the specific NACL (use the returned id from previous output):</p> <p><code>sh aws ec2 describe-network-acls --network-acl-ids acl-062215a2501b5979b</code></p> </li> <li> <p>View NACL entries\\rules     <code>sh     aws ec2 describe-network-acls \\     --network-acl-ids acl-062215a2501b5979b \\     --query \"NetworkAcls[*].Entries[*].{RuleNumber:RuleNumber,Egress:Egress,Protocol:Protocol,RuleAction:RuleAction,CIDR: CidrBlock,Ports:[PortRange.From, PortRange.To]}\" \\     --output table</code></p> </li> </ol>"},{"location":"vpc/nacl/#notes","title":"Notes","text":"<ul> <li>This example uses minimal NACL rules for demonstration. Adjust inbound/outbound rules based on your security requirements.</li> <li>AWS Network ACLs are stateless; ensure rules are mirrored for inbound and outbound traffic where necessary.</li> <li>For production, consider additional layers like security groups and flow logs.</li> </ul>"},{"location":"vpc/nacl/#cleanup","title":"Cleanup","text":"<p>To destroy all resources:</p> <pre><code>terraform destroy\n</code></pre>"}]}